# âš¡ Step 1: What is Time Complexity?

ðŸ‘‰ Time complexity tells us **how the number of steps in an algorithm grows as the input size grows**.

* Input size = usually denoted by **n**.
* We donâ€™t care about exact seconds (because it depends on CPU, compiler, etc.), we care about the **growth trend**.

ðŸ“Œ Example:
Imagine searching a name in a phonebook.

* If you go one by one â†’ **Linear Search = O(n)**.
* If you open the middle and cut the book into half each time â†’ **Binary Search = O(log n)**.

So time complexity helps us compare which method is **faster for large inputs**.

---

# âš¡ Step 2: Counting Steps

We count the **basic operations** (comparisons, additions, multiplications, etc.).

### Example 1:

```cpp
for(int i=0; i<n; i++) {
    cout << i;
}
```

* Loop runs from 0 â†’ n-1
* Number of steps = n
* **Time Complexity = O(n)**

---

### Example 2:

```cpp
for(int i=0; i<n; i++) {
    for(int j=0; j<n; j++) {
        cout << i << j;
    }
}
```

* Outer loop = n times
* Inner loop = n times (for each outer loop)
* Total = n Ã— n = nÂ²
* **Time Complexity = O(nÂ²)**

---

### Example 3:

```cpp
for(int i=1; i<n; i=i*2) {
    cout << i;
}
```

* i = 1, 2, 4, 8, â€¦, until < n
* Doubles each time â†’ runs about logâ‚‚(n) times
* **Time Complexity = O(log n)**

---

# âš¡ Step 3: Big-O, Big-Î©, Big-Î˜

ðŸ‘‰ These are **notations** to describe an algorithmâ€™s behavior.

### 1. **Big-O (O)** â†’ Worst Case

* Maximum steps the algorithm can take.
* Think: *What if things go bad?*

Example:

* Linear Search (finding an element in array)

  * Best Case â†’ found at index 0 â†’ **O(1)**
  * Worst Case â†’ found at last index â†’ **O(n)**
  * We write **O(n)** (worst case).

---

### 2. **Big-Î© (Î©)** â†’ Best Case

* Minimum steps algorithm can take.
* Think: *How good can it get?*

Example:

* Linear Search

  * If element is at index 0 â†’ only 1 comparison.
  * **Î©(1)**

---

### 3. **Big-Î˜ (Î˜)** â†’ Average Case

* The â€œmiddle groundâ€ â†’ how the algorithm performs **on average**.
* Linear Search â†’ on average element is in the middle â†’ Î˜(n/2) â‰ˆ **Î˜(n)**

---

# âš¡ Step 4: Common Complexities (ordered from best to worst)

| Complexity | Name         | Example                           |
| ---------- | ------------ | --------------------------------- |
| O(1)       | Constant     | Accessing an array element        |
| O(log n)   | Logarithmic  | Binary Search                     |
| O(n)       | Linear       | Linear Search                     |
| O(n log n) | Linearithmic | Merge Sort, Quick Sort (avg)      |
| O(nÂ²)      | Quadratic    | Bubble Sort                       |
| O(2â¿)      | Exponential  | Recursive Fibonacci               |
| O(n!)      | Factorial    | Travelling Salesman (brute force) |

---

# âš¡ Step 5: Why Do We Need This?

Imagine you want to sort **1 billion numbers**.

* Bubble Sort (O(nÂ²)) â†’ takes **years**.
* Merge Sort (O(n log n)) â†’ takes **minutes**.

Thatâ€™s why **DAA = Design efficient algorithms + Analyze them**.

---

âœ… Now you know:

* What time complexity means.
* How to calculate using loops.
* The difference between O, Î©, and Î˜.
* Why efficient algorithms matter.

---
